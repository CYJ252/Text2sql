import asyncio
import json
import logging
import re
from typing import Iterator
from openai import OpenAI
import pandas as pd
import requests
import os
import time
import datetime

# å¯¼å…¥promptæ¨¡å—
from .prompt import PROMPTS


class QuerySystem:
    def __init__(self, llm_model, vllm_host ,api_key='Empty'):
        self.llm_model = llm_model
        self.vllm_host = vllm_host
        self.vllm_client =OpenAI(
            base_url=self.vllm_host,
            api_key=api_key,
        )

    
    def query_ck(self, user_question, table_info, case_info,max_retries=3,ck_client=None):
        question = user_question
        retry_count = 0
        table_info_str = json.dumps(table_info, ensure_ascii=False, indent=2)
        while retry_count < max_retries:
            sql=self._generate_sql_candidates(question, table_info_str, case_info)

            
            sql = sql.strip()
            pattern = r'```sql(.*?)```'
            matches = re.findall(pattern, sql, re.DOTALL | re.IGNORECASE)
            sql_query =  matches[0].strip()

            print('---------- SQL ----------')
            print(sql_query)

            try:
                # SQL 1 ç»“æœ
                result = ck_client.query(sql_query)
                result_df = pd.DataFrame(result.result_rows, columns=result.column_names)
                # 2. è½¬æ¢ä¸º Markdown å­—ç¬¦ä¸²
                # index=False è¡¨ç¤ºä¸åŒ…å«è¡Œå·ï¼Œè¿™å¯¹äº LLM é˜…è¯»æ›´å‹å¥½
                markdown_table = result_df.to_markdown(index=False)
                return sql_query, markdown_table

            except Exception as e:
                print(f"âš ï¸ æŸ¥è¯¢ SQL æ—¶å‡ºé”™ (ç¬¬ {retry_count+1} æ¬¡): {e}")
                error_message = str(e)

                # å°†é”™è¯¯åé¦ˆç»™ LLM è®©å…¶ä¿®æ­£
                question =user_question+ f"\nä¸Šä¸€ä¸ª SQL{sql_query} å‡ºé”™ï¼Œé”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š{error_message}\nè¯·ä¿®æ­£å¹¶é‡æ–°ç”Ÿæˆ SQLã€‚"

                retry_count += 1
                if retry_count == max_retries:
                    print("âŒ å¤šæ¬¡é‡è¯•ä»å¤±è´¥ï¼Œåœæ­¢é‡è¯•ã€‚")
                    return sql_query, None

    def query(self, question, results, max_retries=3):
        print(f"\næ­£åœ¨å¤„ç†é—®é¢˜: {question}")

        if not results:
            context_str = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        else:
            context_str = results
        
    
        print("æ­¥éª¤2: ç»„åˆæç¤ºè¯å¹¶è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")
        for attempt in range(max_retries):
            print(f"\n--- ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯• ---")

            # æ­¥éª¤2: ç”Ÿæˆå¹¶æ’åºå€™é€‰SQL
            candidates_text = self._generate_sql_candidates(question, context_str)
            print(candidates_text)
            ranked_sqls = self._parse_and_rank_candidates(question, candidates_text)

            if not ranked_sqls:
                print("æœªèƒ½ç”Ÿæˆä»»ä½•å€™é€‰SQLï¼Œå°è¯•ç»§ç»­...")
                continue
            
            print(f"æ­¥éª¤2.3: è·å¾— {len(ranked_sqls)} ä¸ªæ’åºåçš„å€™é€‰SQLã€‚")

            # æ­¥éª¤3: æ ¡éªŒå¾ªç¯
            for i, sql in enumerate(ranked_sqls):
                print(f"\næ­£åœ¨æ ¡éªŒæ’åç¬¬ {i+1} çš„SQL:")
                print(f"sql\n{sql}\n```")

                    # 3a. è¯­æ³•æ ¡éªŒ
                if not self._validate_sql_syntax(sql):
                    continue # å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª

                # 3b. è¯­ä¹‰æ ¡éªŒ
                if not self._validate_sql_semantics(sql, context_str,question):
                    continue # å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                
                # å¦‚æœæ‰€æœ‰æ ¡éªŒéƒ½é€šè¿‡
                print("\nğŸ‰ æ‰¾åˆ°ä¸€ä¸ªé€šè¿‡æ‰€æœ‰æ ¡éªŒçš„æœ‰æ•ˆSQLï¼")
                final_answer = f"```sql\n{sql}\n```"
                print("\næœ€ç»ˆç­”æ¡ˆ:")
                print(final_answer)
                return final_answer # æˆåŠŸï¼Œè¿”å›ç»“æœå¹¶é€€å‡ºå‡½æ•°

            print("\næœ¬æ¬¡å°è¯•ä¸­çš„æ‰€æœ‰å€™é€‰SQLå‡æœªé€šè¿‡æ ¡éªŒã€‚")

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        print("\næ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„SQLã€‚")
        final_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•ç”Ÿæˆä¸€ä¸ªæœ‰æ•ˆçš„SQLæ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•æ¢ä¸€ç§é—®æ³•æˆ–è”ç³»æŠ€æœ¯äººå‘˜ã€‚"
        print(final_answer)
        return final_answer
    
    def html_query(self, question,sql_1, result1,sql_2, result2,SAVE_PATH=None,number=0):
        html_outpot=self.html_explain_result(question,sql_1, result1,sql_2, result2)
        pattern = r'```html(.*?)```'
        matches = re.findall(pattern, html_outpot, re.DOTALL | re.IGNORECASE)
        if SAVE_PATH is not None:
            with open(f'{SAVE_PATH}/{number}.html', 'w', encoding='utf-8') as f:
                f.write(matches[0].strip())
    
    def html_explain_result(self, question,sql_1, result1,sql_2, result2):
        sys_prompt = PROMPTS["Finall_2"]
        def safe_str(value):
            return "" if value is None else str(value)
        sys_prompt = sys_prompt.replace("{question}", safe_str(question))
        sys_prompt = sys_prompt.replace("{sql_1}", safe_str(sql_1))
        sys_prompt = sys_prompt.replace("{res_1_md}", safe_str(result1))
        sys_prompt = sys_prompt.replace("{sql_2}", safe_str(sql_2))
        sys_prompt = sys_prompt.replace("{res_2_md}", safe_str(result2))

        try:
            res = self.vllm_client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": sys_prompt}],
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
            temperature=0,
            )
            result=res.choices[0].message.content.strip()
            return result

        except Exception as e:
            print(f"\n\nå‘ç”Ÿé”™è¯¯: {e}")
            return None

    def generate_json_analysis_2(self, question,sql_1, result1,sql_2, result2):
       # 1. å‡†å¤‡æ•°æ®ï¼šå°†ç»“æœè½¬ä¸º Markdown æ ¼å¼ï¼Œåˆ©äº LLM ç†è§£
        res_1_md = result1
        res_2_md = result2
        
        # 2. æ›¿æ¢ Prompt å˜é‡
        sys_prompt = PROMPTS["Finall_2"]
        def safe_str(value):
            return "" if value is None else str(value)

        # ä½¿ç”¨ replace è€Œä¸æ˜¯ formatï¼Œé¿å… JSON æ¨¡æ¿ä¸­çš„èŠ±æ‹¬å·å†²çª
        sys_prompt = sys_prompt.replace("{question}", safe_str(question))
        sys_prompt = sys_prompt.replace("{sql_1}", safe_str(sql_1))
        sys_prompt = sys_prompt.replace("{res_1_md}", safe_str(res_1_md))
        sys_prompt = sys_prompt.replace("{sql_2}", safe_str(sql_2))
        sys_prompt = sys_prompt.replace("{res_2_md}", safe_str(res_2_md))
        sys_prompt = sys_prompt.replace("{current_time_str}", datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"))

        try:
            # 3. è°ƒç”¨ LLM
            res = self.vllm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": sys_prompt}],
                extra_body={"chat_template_kwargs": {"enable_thinking": False}},
                temperature=0.1, # ç¨å¾®ç»™ä¸€ç‚¹ç‚¹æ¸©åº¦æˆ–ä¿æŒ0ï¼ŒJSONæ ¼å¼ç”Ÿæˆ0.1é€šå¸¸æ¯”è¾ƒç¨³
            )
            llm_output = res.choices[0].message.content.strip()

            # 4. è§£æ JSON
            parsed_result = self._parse_llm_json(llm_output)

            if parsed_result:
                # ç¡®ä¿ status å­˜åœ¨
                if "status" not in parsed_result:
                    parsed_result["status"] = "success"
                return parsed_result
            else:
                # è§£æå¤±è´¥çš„å…œåº•è¿”å›
                return {
                    "status": "error",
                    "sql": sql_1 or sql_2,
                    "result": None,
                    "message": f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æä¸ºJSONã€‚åŸå§‹å†…å®¹ç‰‡æ®µ: {llm_output[:50]}..."
                }

        except Exception as e:
            print(f"\n\nå‘ç”Ÿé”™è¯¯: {e}")
            return {
                "status": "error",
                "sql": sql_1,
                "result": None,
                "message": f"å†…éƒ¨å¤„ç†é”™è¯¯: {str(e)}"
            }

    def generate_json_analysis(self, question,sql, result):
       # 1. å‡†å¤‡æ•°æ®ï¼šå°†ç»“æœè½¬ä¸º Markdown æ ¼å¼ï¼Œåˆ©äº LLM ç†è§£
        # 2. æ›¿æ¢ Prompt å˜é‡
        sys_prompt = PROMPTS["Finall"]
        def safe_str(value):
            return "None" if (value is None or value == "") else str(value)

        # ä½¿ç”¨ replace è€Œä¸æ˜¯ formatï¼Œé¿å… JSON æ¨¡æ¿ä¸­çš„èŠ±æ‹¬å·å†²çª
        sys_prompt = sys_prompt.replace("{question}", safe_str(question))
        sys_prompt = sys_prompt.replace("{sql}", safe_str(sql))
        sys_prompt = sys_prompt.replace("{query_result}", safe_str(result))
        sys_prompt = sys_prompt.replace("{current_time_str}", datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"))

        with open('logs/ç»“æœåˆ†æprompt.txt', 'w', encoding='utf-8') as f:
            f.write(sys_prompt)

        try:
            # 3. è°ƒç”¨ LLM
            res = self.vllm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": sys_prompt}],
                extra_body={"chat_template_kwargs": {"enable_thinking": False}},
                temperature=0.1, # ç¨å¾®ç»™ä¸€ç‚¹ç‚¹æ¸©åº¦æˆ–ä¿æŒ0ï¼ŒJSONæ ¼å¼ç”Ÿæˆ0.1é€šå¸¸æ¯”è¾ƒç¨³
            )
            llm_output = res.choices[0].message.content.strip()

            # 4. è§£æ JSON
            parsed_result = self._parse_llm_json(llm_output)

            if parsed_result:
                # ç¡®ä¿ status å­˜åœ¨
                if "status" not in parsed_result:
                    parsed_result["status"] = "success"
                return parsed_result
            else:
                # è§£æå¤±è´¥çš„å…œåº•è¿”å›
                return {
                    "status": "error",
                    "sql": sql,
                    "result": None,
                    "message": f"æ¨¡å‹è¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æä¸ºJSONã€‚åŸå§‹å†…å®¹ç‰‡æ®µ: {llm_output[:50]}..."
                }

        except Exception as e:
            print(f"\n\nå‘ç”Ÿé”™è¯¯: {e}")
            return {
                "status": "error",
                "sql": sql,
                "result": None,
                "message": f"å†…éƒ¨å¤„ç†é”™è¯¯: {str(e)}"
            }
        
    def generate_json_analysis_stream(self, question, sql, result) -> Iterator[str]:
        """
        æµå¼ç”Ÿæˆåˆ†æç»“æœ
        """
        # 1. å‡†å¤‡æ•°æ®
        # ä½¿ç”¨ä½ ä¿®æ”¹åçš„ PROMPTS["Finall_2"]
        sys_prompt = PROMPTS["Finall_stream"] 
        
        def safe_str(value):
            return "" if value is None else str(value)

        sys_prompt = sys_prompt.replace("{question}", safe_str(question))
        sys_prompt = sys_prompt.replace("{sql}", safe_str(sql))
        # è¿™é‡Œçš„ result å·²ç»æ˜¯ markdown è¡¨æ ¼å­—ç¬¦ä¸²
        sys_prompt = sys_prompt.replace("{query_result}", safe_str(result))
        sys_prompt = sys_prompt.replace("{current_time_str}", datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"))

        try:
            # 2. è°ƒç”¨ LLMï¼Œå¼€å¯ stream=True
            stream = self.vllm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": sys_prompt}],
                extra_body={"chat_template_kwargs": {"enable_thinking": False}},
                temperature=0.1,
                stream=True  # <--- å…³é”®å¼€å¯æµå¼
            )

            # 3. é€æ­¥ yield å†…å®¹
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content

        except Exception as e:
            print(f"\n\næµå¼ç”Ÿæˆå‘ç”Ÿé”™è¯¯: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œè¿”å›ä¸€ä¸ªç¬¦åˆ JSON ç»“æ„çš„é”™è¯¯ä¿¡æ¯ç‰‡æ®µï¼Œæˆ–è€…ç›´æ¥æŠ›å‡º
            yield f'{{"status": "error", "message": "Stream error: {str(e)}"}}'

    def _generate_sql_candidates(self, question, context_str, case_str):
        """
        ä½¿ç”¨CoTç”Ÿæˆå€™é€‰SQLã€‚
        """
        now = datetime.datetime.now()
        current_time_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
        sys_prompt_temp = PROMPTS["generate_SQL"]
        sys_prompt = sys_prompt_temp.format(
            current_time_str=current_time_str,
            question=question,
            context_str=context_str,
            case_str=case_str,
        )

        with open('logs/SQLç”Ÿæˆprompt.txt', 'w', encoding='utf-8') as f:
            f.write(sys_prompt)

        res = self.vllm_client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": sys_prompt}],
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
            temperature=0,
        )
        result=res.choices[0].message.content.strip()
        return result

    def _parse_and_rank_candidates(self, question, candidates_text):
        """
        è§£æLLMç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¹¶å¯¹SQLè¿›è¡Œæ’åºã€‚
        """
        print("æ­¥éª¤2.2: è§£æå¹¶å¯¹å€™é€‰SQLè¿›è¡Œæ’åº...")
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æå‡ºæ€è€ƒå’ŒSQL
        pattern = re.compile(r"\[CANDIDATE \d+\]\s*æ€è€ƒ: (.*?)\s*SQL:\s*```sql\s*(.*?)\s*```", re.DOTALL)
        matches = pattern.findall(candidates_text)
        
        if not matches:
            print("è­¦å‘Š: æ— æ³•ä»LLMçš„è¾“å‡ºä¸­è§£æå‡ºä»»ä½•å€™é€‰SQLã€‚")
            # å°è¯•æŠŠæ•´ä¸ªè¾“å‡ºå½“ä½œä¸€ä¸ªSQL
            if "```sql" in candidates_text:
                sql_match = re.search(r"```sql\s*(.*?)\s*```", candidates_text, re.DOTALL)
                if sql_match:
                    return [sql_match.group(1).strip()]
            return []

        # å€™é€‰SQLåˆ—è¡¨
        candidate_sqls = [sql.strip() for _, sql in matches]

        # æ„å»ºæ’åºPrompt
        prompt_template = f"""
        ä½ æ˜¯ä¸€ä¸ªSQLè¯„å®¡ä¸“å®¶ã€‚ä¸‹é¢æ˜¯ç”¨æˆ·çš„ä¸€ä¸ªé—®é¢˜å’Œå‡ ä¸ªç”±AIç”Ÿæˆçš„å€™é€‰SQLã€‚æŒ‰åŸæ¥å¾—åˆ°é¡ºåºå¯¹å®ƒä»¬è¿›è¡Œæ’åºã€‚

        [ç”¨æˆ·é—®é¢˜]
        {question}

        [å€™é€‰SQLåˆ—è¡¨]
        """
        for i, sql in enumerate(candidate_sqls):
            prompt_template += f"\n-- SQL {i+1} --\n{sql}\n"

        prompt_template += """
        # ä»»åŠ¡:
        è¯·è¾“å‡ºä¸€ä¸ªæ’åºåçš„ç´¢å¼•åˆ—è¡¨ï¼ŒæŒ‰åŸæ¥çš„é¡ºåºè¿›è¡Œæ’åº
        åªè¾“å‡ºæ•°å­—å’Œé€—å·ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šã€‚

        æ’åºç´¢å¼•:
        """
        res = self.vllm_client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": prompt_template}],
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
            temperature=0,
        )
        response=res.choices[0].message.content.strip()
        
        try:
            order_str = response['message']['content'].strip()
            # æ¸…ç†å¯èƒ½çš„éæ•°å­—å­—ç¬¦
            order_str = re.sub(r'[^\d,]', '', order_str)
            ranked_indices = [int(i.strip()) - 1 for i in order_str.split(',')]
            
            # æ ¹æ®LLMè¿”å›çš„é¡ºåºé‡æ–°æ’åˆ—SQL
            ranked_sqls = [candidate_sqls[i] for i in ranked_indices if i < len(candidate_sqls)]
            
            # æ·»åŠ ä»»ä½•æœªè¢«æ’åºçš„SQLåˆ°æœ«å°¾ï¼Œä»¥é˜²LLMæ’åºå‡ºé”™
            for i, sql in enumerate(candidate_sqls):
                if sql not in ranked_sqls:
                    ranked_sqls.append(sql)

            print(f"LLMæ’åºç»“æœ: {order_str}")
            return ranked_sqls
        except Exception as e:
            print(f"æ’åºå¤±è´¥: {e}ã€‚å°†ä½¿ç”¨åŸå§‹é¡ºåºã€‚")
            return candidate_sqls # å¦‚æœæ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹é¡ºåº
        

    def _validate_sql_semantics(self, sql_query, context_str, question):
        """
        ä½¿ç”¨LLMæ£€æŸ¥SQLçš„é€»è¾‘ã€‚
        """
        sys_prompt_temp = PROMPTS["validate_sql"]
        sys_prompt = sys_prompt_temp.format(
            sql_query=sql_query,
            context_str=context_str,
        )

        res = self.vllm_client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": sys_prompt}],
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
            temperature=0,
        )
        answer = res.choices[0].message.content.strip()
        if answer.upper() == "OK":
            print("  âœ… è¯­ä¹‰æ ¡éªŒé€šè¿‡")
            return True
        else:
            print(f"  âŒ è¯­ä¹‰æ ¡éªŒå¤±è´¥: {answer}")
            return False 

    # è¾…åŠ©æ–¹æ³•ï¼šè§£æ LLM è¿”å›çš„ JSON å­—ç¬¦ä¸²
    def _parse_llm_json(self, text):
        try:
            # 1. å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        try:
            # 2. ä½¿ç”¨æ­£åˆ™æå–ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹ï¼ˆå»é™¤ markdown ä»£ç å—æ ‡è®°ï¼‰
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
        except Exception:
            pass
        
        # 3. è§£æå¤±è´¥è¿”å› None
        return None
