#åˆ©ç”¨å¤§æ¨¡å‹ï¼Œæ ¹æ®è¡¨ä¸­çš„å‡ ä¸ªå®ä¾‹ï¼Œå¾—åˆ°æ¯ä¸ªå­—æ®µçš„å–å€¼çš„æè¿°
from clickhouse_connect import get_client
import json
import re
from openai import OpenAI
from datetime import datetime

from config import Config

# INDEX_NAME = "oh_report_anal_daily_ex"
DOC_LIMIT = 10  # æŠ“å–æ–‡æ¡£æ•°é‡
# =====================
# åˆ›å»º CK å®¢æˆ·ç«¯
# =====================
client = get_client(
    host=Config.CK_HOST,
    port=Config.CK_PORT,
    username=Config.CK_USERNAME,
    password=Config.CK_PASSWORD,
    database=Config.CK_DATABASE
)
# =====================
# åˆ›å»º OpenAI å®¢æˆ·ç«¯
# =====================
# LLM å®¢æˆ·ç«¯
openai_client = OpenAI(
    base_url=Config.VLLM_HOST,
    api_key="EMPTY",
)
llm_model = Config.LLM_MODEL

def convert_dict_to_text(obj):
    if isinstance(obj, dict):
        return {key: convert_dict_to_text(value) for key, value in obj.items()}
    elif isinstance(obj, datetime):
        return obj.isoformat()  # ä½¿ç”¨ ISO 8601 æ ¼å¼
    else:
        return str(obj)  # å°†å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²

# =====================
# é€šç”¨å·¥å…·å‡½æ•°
# =====================
def char_list_to_bracket_string(char_list):
    return ",".join([f"[{ch}]" for ch in char_list])

def generate_table_field_comment_json(table_name, field_comment_map):
    """
    ç”Ÿæˆå¸¦è¡¨åçš„ JSON ç»“æ„
    """
    if not isinstance(field_comment_map, dict):
        raise ValueError("âŒ è¾“å…¥å­—æ®µå¿…é¡»ä¸º dict æ ¼å¼ï¼Œå¦‚ {'å­—æ®µå': 'æ³¨é‡Š'}")
    result = {
        "table_name": table_name,
        "fields": field_comment_map
    }
    return json.dumps(result, ensure_ascii=False, indent=2)

# =====================
# ä» ClickHouse æŠ“å–æ ·æœ¬æ–‡æ¡£
# =====================
def fetch_documents(table_name, size=10):
    try:
        query = f"SELECT * FROM {table_name} LIMIT {size}"
        result = client.query(query)
        columns = result.column_names
        docs = [dict(zip(columns, row)) for row in result.result_rows]

        # è¿‡æ»¤ï¼šä»…ä¿ç•™å•æ¡é•¿åº¦å°äº max_chars çš„è®°å½•
        max_chars = 1000
        filtered_docs = []
        for doc in docs:
            json_str = convert_dict_to_text(doc)
            total_length = 0
            for key, value in json_str.items():
                if value is not None:  # æ’é™¤Noneå€¼
                    total_length += len(str(value))
                    #print(total_length)

            if total_length <= max_chars:
                filtered_docs.append(doc)

        return filtered_docs

    except Exception as e:
        print(f"æŠ“å–è¡¨ {table_name} æ–‡æ¡£å¤±è´¥: {e}")
        return []

# =====================
# è§£æå»ºè¡¨è¯­å¥ï¼ˆç®€å•ç‰ˆï¼‰
# =====================
def parse_create_table(sql: str):
    result = {
        "table_name": None,
        "columns": [],
        "primary_keys": []
    }

    # 1. æå–è¡¨å
    table_match = re.search(r"CREATE\s+TABLE\s+`?(\w+)`?", sql, re.IGNORECASE)
    if table_match:
        result["table_name"] = table_match.group(1)

    # æå–è¡¨æ³¨é‡Š
    table_comment_match = re.search(r"COMMENT\s*=\s*'([^']+)'", sql, re.IGNORECASE)
    if table_comment_match:
        result["table_comment"] = table_comment_match.group(1)

    # 2. æå–å­—æ®µå®šä¹‰ï¼ˆæ‹¬å·å†…å†…å®¹ï¼‰
    fields_section = re.search(r"\((.*)\)", sql, re.S)
    if not fields_section:
        return result
    fields_text = fields_section.group(1)

    # æŒ‰è¡Œåˆ†å‰²å­—æ®µ
    lines = [line.strip().strip(",") for line in fields_text.split("\n") if line.strip()]

    for line in lines:
        # è·³è¿‡è¡¨çº§çº¦æŸï¼ˆPRIMARY KEY ç­‰ï¼‰
        if line.upper().startswith("PRIMARY KEY"):
            pk_match = re.findall(r"`(\w+)`", line)
            result["primary_keys"].extend(pk_match)
            continue
        if line.upper().startswith("KEY"):
            continue  # è·³è¿‡ç´¢å¼•å®šä¹‰

        # æå–å­—æ®µå®šä¹‰ï¼ˆå…¼å®¹å¸¦å¼•å·å’Œä¸å¸¦å¼•å·çš„COMMENTï¼‰
        field_match = re.match(
            r"`(?P<name>\w+)`\s+(?P<type>[^'\s]+)(.*?COMMENT\s+['\"]?(?P<comment>[^'\"]*)['\"]?)?",
            line,
            re.IGNORECASE
        )
        if field_match:
            col = {
                "name": field_match.group("name"),
                "type": field_match.group("type"),
                "comment": field_match.group("comment") if field_match.group("comment") else "",
                "not_null": "NOT NULL" in line.upper()
            }
            result["columns"].append(col)

            # å¦‚æœæ˜¯è¡Œå†…ä¸»é”®ï¼ˆä¾‹å¦‚ `id` INT PRIMARY KEYï¼‰
            if "PRIMARY KEY" in line.upper():
                result["primary_keys"].append(col["name"])

    return result

# =====================
# æ„é€ å¤§æ¨¡å‹ Prompt
# =====================
def generate_prompt(table_name, chinese_name, key_words, documents, english_chinese_field):
    docs_str = json.dumps(documents, ensure_ascii=False, indent=2)
    prompt = f"""
    ä½ æ˜¯ä¸€åç²¾é€š ClickHouse çš„æ•°æ®åº“æ–‡æ¡£ä¸“å®¶ã€‚è¯·æ ¹æ®æ‰€æä¾›çš„è¡¨ç»“æ„ä¿¡æ¯å’Œç¤ºä¾‹æ•°æ®ï¼Œç”Ÿæˆä¸€ä¸ª JSON å¯¹è±¡ï¼Œç”¨äºæè¿°è¯¥è¡¨æ¯ä¸ªå­—æ®µçš„å–å€¼ç‰¹æ€§ã€‚

    è¡¨è‹±æ–‡åï¼š{table_name}
    è¡¨ä¸­æ–‡åï¼š{chinese_name}
    æ ¸å¿ƒå…³é”®å­—ï¼š{key_words}
    å­—æ®µåç§°ä¸ä¸­æ–‡æ³¨é‡Šï¼š{english_chinese_field}
    è¡¨ç¤ºä¾‹æ•°æ®ï¼ˆæ¯æ¡ä¸º JSON å¯¹è±¡ï¼‰ï¼š
    {docs_str}

    è¾“å‡ºè¦æ±‚ï¼š

    1. è¾“å‡ºä¸º JSON å¯¹è±¡ï¼Œç»“æ„å¦‚ä¸‹ï¼š
    {{
      "table_name": "{table_name}",
      "chinese_name": "{chinese_name}",
      "fields": {{
        "å­—æ®µè‹±æ–‡å1": "æè¿°å­—æ®µå–å€¼ç±»å‹ï¼ˆæ–‡å­—ã€æ•°å­—ã€å…³é”®å­—ã€æ—¶é—´ç­‰ï¼‰ã€ä¸€èˆ¬é•¿åº¦ã€å…¸å‹æ ¼å¼åŠç‰¹æ€§",
        "å­—æ®µè‹±æ–‡å2": "åŒä¸Š",
        ...
      }}
    }}

    2. æ¯ä¸ªå­—æ®µå¯¹åº”ä¸€æ®µè‡ªç„¶è¯­è¨€ä¸­æ–‡è¯´æ˜ï¼Œä¸“æ³¨æè¿°å­—æ®µçš„å–å€¼ç±»å‹ã€ä¸€èˆ¬é•¿åº¦ã€æ ¼å¼æˆ–å…¸å‹å–å€¼ï¼Œå­—æ•°ä¸å°‘äº 30 å­—ã€‚
    3. åŸºäºç¤ºä¾‹æ•°æ®æ¨æ–­å­—æ®µçš„å–å€¼ç±»å‹ã€é•¿åº¦èŒƒå›´å’Œæ ¼å¼ç‰¹å¾ï¼Œä½†ç¦æ­¢ç›´æ¥åˆ—å‡ºç¤ºä¾‹æ•°æ®ã€‚
    4. è¾“å‡ºå†…å®¹ä»…ä¸ºç»“æ„åŒ– JSONï¼Œä¸¥ç¦ SQLã€åŸå§‹ JSON æ•°æ®æˆ–å­—æ®µåˆ—è¡¨ã€‚
    5. æ–‡æœ¬æè¿°åº”ç®€æ´ã€ä¸“ä¸šã€æ˜“æ‡‚ï¼Œå¯ç›´æ¥ç”¨äºæ•°æ®å­—å…¸æˆ–æŠ€æœ¯æ–‡æ¡£ã€‚
    6. è¯´æ¸…æ¥šæ˜¯å¦ä¸ºçº¯æ•°å­—æˆ–å­—æ¯å’Œæ•°å­—çš„ç»„åˆã€‚
    è¯·ç›´æ¥è¾“å‡ºç¬¦åˆè¦æ±‚çš„ JSON æ–‡æ¡£ã€‚
    """
    return prompt

# =====================
# è°ƒç”¨ Ollama
# =====================
# def call_ollama(prompt, model="qwen3-30b-instruct"):
#     try:
#         client = ollama.Client()
#         response = client.generate(model=model, prompt=prompt)
#         return response['response']
#     except Exception as e:
#         print(f"è°ƒç”¨ Ollama å¤±è´¥: {e}")
#         return None

def call_openAI(prompt, model="qwen30ba3", enable_thinking=False):
    try:
        response = openai_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            extra_body={"chat_template_kwargs": {"enable_thinking": enable_thinking}},
            temperature=0,
            top_p=1,
        )
        response = response.choices[0].message.content.strip()
        return response
    except Exception as e:
        print(f"è°ƒç”¨ OpenAI å¤±è´¥: {e}")
        return None

# =====================
# ä¸»ç¨‹åºå…¥å£
# =====================
if __name__ == "__main__":

    TableDescribetionDict = {}

    # è¿™é‡Œå‡è®¾å­˜åœ¨ä¸€ä¸ª metadata è¡¨ï¼ˆæˆ– JSON æ–‡ä»¶ï¼‰ä¿å­˜æ¯ä¸ªä¸šåŠ¡è¡¨çš„å…ƒä¿¡æ¯
    # ä¾‹å¦‚ï¼štable_metadata(è‹±æ–‡è¡¨å, ä¸­æ–‡è¡¨å, ä½¿ç”¨åœºæ™¯, è¡¨å…³é”®å­—, å»ºè¡¨è¯­å¥)
    meta_query = f"""SELECT table_name_en, table_name_cn, business_module_lvl1, business_module_lvl2, create_sql, related_tables, table_keywords FROM "default"."table_meta" """
    meta_rows = client.query(meta_query).result_rows

    file_path = "Table_EachField_Descrition_Value.txt"
    All_Text = []
    #test_count = 0
    for row in meta_rows:
        # test_count = test_count + 1
        # if test_count > 5:
        #      break
        INDEX_NAME, INDEX_NAME_CHINESE, BUSSINESS_MODULE_LV1, BUSSINESS_MODULE_LV2, CREATE_SQL, RELATED_TABLES, INDEX_KEY_WORD = row

        print(f"å¤„ç†è¡¨ï¼š{INDEX_NAME} ...")
        # è§£æå»ºè¡¨è¯­å¥
        fields_info = parse_create_table(CREATE_SQL)

        Chinese_English_dict = {col["name"]: col["comment"] for col in fields_info["columns"]}
        Chinese_Field_String = char_list_to_bracket_string(list(Chinese_English_dict.values()))
        Chinese_English_Sentence = generate_table_field_comment_json(INDEX_NAME, Chinese_English_dict)

        # æŠ“æ ·æœ¬æ–‡æ¡£
        docs = fetch_documents(INDEX_NAME, DOC_LIMIT)
        if not docs:
            print(f"âš ï¸ è¡¨ {INDEX_NAME} æœªæŠ“å–åˆ°æ ·æœ¬æ•°æ®ï¼Œè·³è¿‡")
            continue

        #jsonDoc = []
        #for eachdoc in docs:
        #    jsonDoc.append(json.dumps(eachdoc))

        jsonDoc = []
        for eachdoc in docs:
            converted_data = convert_dict_to_text(eachdoc)
            jsonDoc.append(json.dumps(converted_data))

        # æ„é€  Prompt
        prompt = generate_prompt(
            INDEX_NAME,
            INDEX_NAME_CHINESE,
            INDEX_KEY_WORD,
            jsonDoc,
            Chinese_English_Sentence
        )

        # è°ƒç”¨æ¨¡å‹
        #result = call_ollama(prompt, model="qwen3:latest")
        byte_length = len(prompt.encode("utf-8"))
        print("UTF-8 å­—èŠ‚é•¿åº¦:", byte_length)

        # è®¡ç®—
        result = call_openAI(prompt,model=llm_model)
        print(result)

        TableDescribetionDict[INDEX_NAME] = result

        if not result:
            continue

        clean_text = re.sub(r"<think>.*?</think>", "", result, flags=re.DOTALL).strip()
        All_Text.append(clean_text)
        print(f"âœ… å·²ç”Ÿæˆï¼š{INDEX_NAME}")

    # ä¿å­˜è¾“å‡º
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(TableDescribetionDict, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ æ‰€æœ‰è¡¨çš„æè¿°ç”Ÿæˆå®Œæ¯•ï¼è¾“å‡ºè·¯å¾„ï¼š", file_path)
